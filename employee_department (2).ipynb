{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		},
		"toc-autonumbering": false
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Employee Departement Assignment",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 60\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport pyspark.sql.functions as F\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='employee-department', table_name='employee_department_csv')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- employee_id: long\n|-- employee_name: string\n|-- department: string\n|-- state: string\n|-- salary: long\n|-- age: long\n|-- bonus: long\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\ndf.show(5)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+-----------------+----------+-----+------+---+-----+\n|employee_id|    employee_name|department|state|salary|age|bonus|\n+-----------+-----------------+----------+-----+------+---+-----+\n|       1000|        Nitz Leif| Marketing|   CA|  6131| 26|  543|\n|       1001|  Melissia Dedman|   Finance|   AK|  4027| 43| 1290|\n|       1002|Rudolph Barringer|        HR|   LA|  3122| 43| 1445|\n|       1003|      Tamra Amber|  Accounts|   AK|  5717| 47| 1291|\n|       1004|      Mullan Nitz|Purchasing|   CA|  5685| 34| 1394|\n+-----------+-----------------+----------+-----+------+---+-----+\nonly showing top 5 rows\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create a temporary view for SQL queries\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df.createOrReplaceTempView('employee')",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 1:  Find the total number of employees in the company",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Dataframe\ntotal_employees = df.count()\nprint(f\"The total number of employees is: {total_employees}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "The total number of employees is: 1000\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#SQL\ntotal_employees_sql = spark.sql('select count(*) from employee').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+\n|count(1)|\n+--------+\n|    1000|\n+--------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 2: Find the total number of departments in the company",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#DataFrame\ntotal_department = df.select('department').distinct().count()\nprint(f'The total number of department is:{total_department}')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "The total number of department is:6\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "total_department_sql = spark.sql('select count(distinct department) from employee').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------------+\n|count(DISTINCT department)|\n+--------------------------+\n|                         6|\n+--------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 3: Find the department names of the company",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dep_name = df.select('department').distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+\n|department|\n+----------+\n|     Sales|\n| Marketing|\n|   Finance|\n|        HR|\n|  Accounts|\n|Purchasing|\n+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dep_name_sql = spark.sql('select distinct department from employee').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+\n|department|\n+----------+\n|     Sales|\n| Marketing|\n|   Finance|\n|        HR|\n|  Accounts|\n|Purchasing|\n+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 4: Find the total number of employees in each department",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.groupby('department').count().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|  169|\n| Marketing|  170|\n|   Finance|  162|\n|        HR|  171|\n|  Accounts|  162|\n|Purchasing|  166|\n+----------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql('select department, count(*) from employee group by 1').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------+\n|department|count(1)|\n+----------+--------+\n|     Sales|     169|\n| Marketing|     170|\n|   Finance|     162|\n|        HR|     171|\n|  Accounts|     162|\n|Purchasing|     166|\n+----------+--------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 5: Find the total number of employees in each state",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.groupby('state').count().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----+-----+\n|state|count|\n+-----+-----+\n|   CA|  205|\n|   LA|  205|\n|   NY|  173|\n|   AK|  209|\n|   WA|  208|\n+-----+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql('select state, count(*) from employee group by 1').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----+--------+\n|state|count(1)|\n+-----+--------+\n|   CA|     205|\n|   LA|     205|\n|   NY|     173|\n|   AK|     209|\n|   WA|     208|\n+-----+--------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 6: Find the total number of employees in each state of each department",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.groupby('state', 'department').count().show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----+----------+-----+\n|state|department|count|\n+-----+----------+-----+\n|   CA|  Accounts|   35|\n|   CA| Marketing|   33|\n|   LA| Marketing|   26|\n|   WA|  Accounts|   27|\n|   CA|        HR|   28|\n|   CA|Purchasing|   32|\n|   WA|     Sales|   27|\n|   AK|  Accounts|   37|\n|   CA|     Sales|   42|\n|   AK|        HR|   25|\n+-----+----------+-----+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql('select state, department, count(*) from employee group by 1, 2').show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----+----------+--------+\n|state|department|count(1)|\n+-----+----------+--------+\n|   CA|  Accounts|      35|\n|   CA| Marketing|      33|\n|   LA| Marketing|      26|\n|   WA|  Accounts|      27|\n|   CA|        HR|      28|\n+-----+----------+--------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 7: Find the min and max salaries in each department and sort salaries in ascending order",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.groupBy(\"Department\").agg(\n    F.min(\"salary\"),\n    F.max(\"salary\")\n).orderBy(\"min(salary)\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-----------+-----------+\n|Department|min(salary)|max(salary)|\n+----------+-----------+-----------+\n|   Finance|       1006|       9899|\n|  Accounts|       1007|       9890|\n|        HR|       1013|       9982|\n| Marketing|       1031|       9974|\n|     Sales|       1103|       9982|\n|Purchasing|       1105|       9985|\n+----------+-----------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql('select department, min(salary), max(salary) from employee group by 1').show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-----------+-----------+\n|department|min(salary)|max(salary)|\n+----------+-----------+-----------+\n|     Sales|       1103|       9982|\n| Marketing|       1031|       9974|\n|   Finance|       1006|       9899|\n|        HR|       1013|       9982|\n|  Accounts|       1007|       9890|\n+----------+-----------+-----------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 8: Find the names of employees working in NY state under Finance department whose bonuses are greater than the average bonuses of employees in NY state",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "average_bonus_ny = df.filter(df.state == \"NY\").agg({'bonus':'avg'}).collect()[0]['avg(bonus)']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 80,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "average_bonus_ny",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 81,
			"outputs": [
				{
					"name": "stdout",
					"text": "1251.3468208092486\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.filter( (df['state'] == 'NY') & \n           (df['department'] == 'Finance' ) & \n            (df['bonus'] > average_bonus_ny)). \\\n            select('employee_name').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 82,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+\n|       employee_name|\n+--------------------+\n|       Vivan Sifford|\n|      Herder Gallman|\n|          Nena Rocha|\n|       Leif Lemaster|\n|Ellingsworth Meli...|\n|        Escoto Gilma|\n|     Georgeanna Laub|\n|     Durio Tenenbaum|\n|       Juliana Grigg|\n|        Tiffani Benz|\n|          Nitz Ilana|\n|   Phylicia Antonina|\n|         Durio Janey|\n|       Melissia Jere|\n|      Yukiko Kreamer|\n|      Nena Kensinger|\n|      Antonina Ilana|\n+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\n  \"\"\"select employee_name \n    from employee \n    where state = 'NY' \n    and department = 'Finance' \n    and bonus > ( \n    select avg(bonus) from employee where state = 'NY'\n    )\n    \"\"\"\n).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+\n|       employee_name|\n+--------------------+\n|       Vivan Sifford|\n|      Herder Gallman|\n|          Nena Rocha|\n|       Leif Lemaster|\n|Ellingsworth Meli...|\n|        Escoto Gilma|\n|     Georgeanna Laub|\n|     Durio Tenenbaum|\n|       Juliana Grigg|\n|        Tiffani Benz|\n|          Nitz Ilana|\n|   Phylicia Antonina|\n|         Durio Janey|\n|       Melissia Jere|\n|      Yukiko Kreamer|\n|      Nena Kensinger|\n|      Antonina Ilana|\n+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Query 9: Create DF of all those employees whose age is greater than 45 and save them in a file",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### When you use the write.csv method in PySpark, it saves the output in multiple part files within a directory. To save the output as a single CSV file with a meaningful name, you need to first coalesce the DataFrame into a single partition and then save it with the desired filename.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.filter(df['age'] > 45).write.csv( 's3://beaconfire-zhuoya/emp_45_df',  header=True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 96,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"SELECT * FROM employee WHERE Age > 45\").write.csv(\"s3://beaconfire-zhuoya/emp_45_sql\", header=True)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 97,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Employees whose age is greater than 45\nemployees_age_above_45 = df.filter(df.age > 45)\n\n# Coalesce into a single partition and save as a single CSV file\noutput_path = \"s3://beaconfire-zhuoya/employees_age_above_45_df.csv\"\nemployees_age_above_45.coalesce(1).write.mode('overwrite').csv(output_path, header=True)\n\n# Rename the part file to the desired file name\nimport boto3\n\ns3 = boto3.client('s3')\n\nbucket = 'beaconfire-zhuoya'\nprefix = 'employees_age_above_45_df.csv'\nresponse = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\nfor obj in response['Contents']:\n    if obj['Key'].endswith('.csv'):\n        copy_source = {'Bucket': bucket, 'Key': obj['Key']}\n        s3.copy_object(CopySource=copy_source, Bucket=bucket, Key='employees_age_above_45_df.csv')\n        s3.delete_object(Bucket=bucket, Key=obj['Key'])\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Employees whose age is greater than 45 using SQL\nemployees_age_above_45_sql = spark.sql(\"SELECT * FROM employee WHERE age > 45\")\n\n# Coalesce into a single partition and save as a single CSV file\noutput_path_sql = \"s3://beaconfire-zhuoya/employees_age_above_45_sql.csv\"\nemployees_age_above_45_sql.coalesce(1).write.mode('overwrite').csv(output_path_sql, header=True)\n\n# Rename the part file to the desired file name\nresponse = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\nfor obj in response['Contents']:\n    if obj['Key'].endswith('.csv'):\n        copy_source = {'Bucket': bucket, 'Key': obj['Key']}\n        s3.copy_object(CopySource=copy_source, Bucket=bucket, Key='employees_age_above_45_sql.csv')\n        s3.delete_object(Bucket=bucket, Key=obj['Key'])\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		}
	]
}